{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: white; display: fill; border-radius: 5px; background-color: #3ac9f0; font-size: 250%; font-family: serif; letter-spacing: 0.5px\">\n",
    "    <p style=\"padding: 20px; color: white; font-size: 30px\">\n",
    "        <span style=\"font-family:serif;\">\n",
    "                Telco Customer Churn Prediction with Business Rationale\n",
    "        </span>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"600px\" align=\"center\" alt=\"Churn Idea\" src=\"https://fullstar.cloudcircus.jp/dcms_media/image/column_churn.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers are the core of businesses. Without customers, no company can survive in the fast-paced business environment. That is the reason many organisations are building relationships with their customers. As part of Customer Relationship Management, retaining existing customers not only keep their engagement but also increases their loyalty to the company.\n",
    "\n",
    "A customer is classified as churned if one did not use the company's offering for a certain period. In a given period, the penetration of churned customers to the total number of customers is considered as customer churn rate. The churn rate is often used as a Key Performance Indicator for monitoring business performance. The lower the better customer retention (sometimes used to evaluate customer loyalty).\n",
    "\n",
    "Companies may consider sending Electronic Direct Messages or discount vouchers to customers for avoiding the churn rate increment. Sending these marketing promotions to all the existing customers would be the best option to engage customers, and seems to predict customer churn does not have any obvious effect on it. Sometimes, however, these actions take costs. As a business strategist, balancing or increasing Return on Investment is the major task. If we could find customers who intend to stop the services, targeting customers at high risk of churn can reduce unnecessary resources on other customers while attracting customers at high risk of churn to re-engage with the company's offerings.\n",
    "\n",
    "This notebook will implement customer churn prediction on a Telco and explain the rationale behind each step.\n",
    "\n",
    "If you liked this notebook, consider upvoting. Your upvote does encourage us to work more on it. Also, appreciate it if you could leave a comment on the work for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:04.285641Z",
     "iopub.status.busy": "2022-09-12T12:47:04.285159Z",
     "iopub.status.idle": "2022-09-12T12:47:05.999147Z",
     "shell.execute_reply": "2022-09-12T12:47:05.997929Z",
     "shell.execute_reply.started": "2022-09-12T12:47:04.285551Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(852)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Telco Customer Data Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.002806Z",
     "iopub.status.busy": "2022-09-12T12:47:06.002293Z",
     "iopub.status.idle": "2022-09-12T12:47:06.065475Z",
     "shell.execute_reply": "2022-09-12T12:47:06.064342Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.002757Z"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "df = pd.read_csv(f\"../input/telco-customer-churn/{file_name}\")\n",
    "\n",
    "nrow, ncol = df.shape\n",
    "print(f\"File `{file_name}` has {ncol:,} columns with {nrow:,} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.066905Z",
     "iopub.status.busy": "2022-09-12T12:47:06.066539Z",
     "iopub.status.idle": "2022-09-12T12:47:06.137193Z",
     "shell.execute_reply": "2022-09-12T12:47:06.135825Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.066862Z"
    }
   },
   "outputs": [],
   "source": [
    "desc_dtype = df.dtypes\n",
    "desc_iqr = df.describe() \\\n",
    "    .T[[\"min\", \"25%\", \"50%\", \"75%\", \"max\"]] \\\n",
    "    .astype(str) \\\n",
    "    .apply(list, axis=1)\n",
    "desc_uniq = df.select_dtypes(include=\"object\") \\\n",
    "    .apply(pd.Series.unique)\n",
    "\n",
    "desc = pd.concat([desc_iqr, desc_uniq])\n",
    "desc = pd.concat([desc_dtype, desc], axis=1)\n",
    "desc.columns = [\"data type\", \"description\"]\n",
    "desc.index.name = \"column name\"\n",
    "\n",
    "desc = desc.reset_index() \\\n",
    "    .groupby([\"data type\", \"column name\"])[\"description\"] \\\n",
    "    .apply(list)\n",
    "\n",
    "display(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive into the prediction, it is worthwhile to check out the data we got from an external source.\n",
    "\n",
    "The dataset contains 7,043 customers with 21 attributes. According to the data description, the dataset contains 4 types of information.\n",
    "1. Demographic Information\n",
    "2. Account Information\n",
    "3. Services Information\n",
    "4. Customer Status\n",
    "\n",
    "From the above output, we noticed that there are 2 variables that have mismatched data type\n",
    "* `SeniorCitizen` valued at numeric on 0 and 1\n",
    "* `TotalCharges` valued as character numeric\n",
    "\n",
    "Updating the above data type issues allows us to use data information properly. Let us do it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Cleansing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.140255Z",
     "iopub.status.busy": "2022-09-12T12:47:06.139389Z",
     "iopub.status.idle": "2022-09-12T12:47:06.182853Z",
     "shell.execute_reply": "2022-09-12T12:47:06.181715Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.140208Z"
    }
   },
   "outputs": [],
   "source": [
    "check = df[\"TotalCharges\"].str.contains(r\"^(?!.\\d)\", regex=True)\n",
    "display(df[check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in why `TotalCharges` is stored as an object type. We found that there were some customers valued as \" \" in the column, which is also known as missing data. There are many approaches to impute the missing value, but we will fill it with numeric values of zero.\n",
    "\n",
    "\"_Customers who left within the last month â€“ the column is called Churn_\" quoting from the data description.\n",
    "\n",
    "Now we know that the data was captured a month after the customer behaved. The available attribute also came from the \"last\" month. Interestingly, all customers valued at 0 on `tenure` and \"No\" in `Churn`. In other words, these customers are new to the company's offerings and did not pay any fees before. So, we should only impute `TotalCharges` with numeric values of zero based on the business content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.188091Z",
     "iopub.status.busy": "2022-09-12T12:47:06.187618Z",
     "iopub.status.idle": "2022-09-12T12:47:06.200564Z",
     "shell.execute_reply": "2022-09-12T12:47:06.199365Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.188057Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = \"SeniorCitizen\"\n",
    "check = df[dc].astype(bool)\n",
    "df[dc] = np.where(check, \"Yes\", \"No\")\n",
    "\n",
    "dc = \"TotalCharges\"\n",
    "df[dc] = pd.to_numeric(df[dc], errors=\"coerce\").fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integrity Check\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you 100% trust the dataset given by others? We have faced many situations of data errors and misinterpretation in real life, and become a bit paranoid about others one's work. Just a few lines of code, we believe it is rewarding to undergo a data integrity check on the data assumption ~~although datasets available in Kaggle are less likely to have data inconsistency issues~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.203136Z",
     "iopub.status.busy": "2022-09-12T12:47:06.202165Z",
     "iopub.status.idle": "2022-09-12T12:47:06.317167Z",
     "shell.execute_reply": "2022-09-12T12:47:06.316003Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.203100Z"
    }
   },
   "outputs": [],
   "source": [
    "# check if NULL values present\n",
    "assert df.isna().sum().sum() == 0\n",
    "\n",
    "# check if customer is unique per record\n",
    "assert df.shape[0] == df[\"customerID\"].nunique()\n",
    "\n",
    "# company's offerings\n",
    "service_main = [\"PhoneService\", \"InternetService\",]\n",
    "sub_serv_phone = [\"MultipleLines\",]\n",
    "sub_serv_internet = [\n",
    "    \"OnlineSecurity\", \"OnlineBackup\",\n",
    "    \"DeviceProtection\", \"TechSupport\",\n",
    "    \"StreamingTV\", \"StreamingMovies\",\n",
    "]\n",
    "service = service_main + sub_serv_phone + sub_serv_internet\n",
    "\n",
    "# check if customer without services\n",
    "assert sum(df[service].apply(lambda y: sum(\"No\" in col for col in y), axis=1) == len(service)) == 0\n",
    "\n",
    "# check if phone sub-service have phone service\n",
    "assert all(df.query(\"PhoneService == 'No'\")[sub_serv_phone] == \"No phone service\")\n",
    "\n",
    "# check if internet sub-service have internet service\n",
    "assert all(df.query(\"InternetService == 'No'\")[sub_serv_internet] == \"No internet service\")\n",
    "\n",
    "print(\"Data integrity check PASSED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.319192Z",
     "iopub.status.busy": "2022-09-12T12:47:06.318859Z",
     "iopub.status.idle": "2022-09-12T12:47:06.323512Z",
     "shell.execute_reply": "2022-09-12T12:47:06.322581Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.319161Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.to_excel(\"tcc_data_after_cleansing.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the dataset is important to understand how data is distributed and behaved. Data visualisation is indeed a great tool to perform exploratory data analysis. Personally, creating fancy visualisation in Python is not very user-friendly and time-consuming. Rather than building charts in the notebook, please refer to the work on [Tableau Public](https://public.tableau.com/app/profile/jackcky/viz/TCC_EDA/EDA).\n",
    "\n",
    "Key findings on Exploratory Data Analysis:\n",
    "* About 27% of customers stopped using the services; implying a significant imbalance in response variable `Churn`\n",
    "* Some attributes showed customers are less likely to churn, such as `Partner`, `Dependents`, `Contract` and `InternetService`\n",
    "* The shorter period (in months) of service usage, the more likely to churn\n",
    "* Customers used the services for a long usually have a lower `MonthlyCharges`; indicating a good measuring for customer loyalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have explanatory variables that are a linear combination of the response variable, we could build a quite decent machine learning model. However, we do not have such variables. We can create some additional attributes to describe more about each customer, such that the machine learning models know how to figure out which customers are likely to churn. We gained some insights from the Exploratory Data Analysis and inspired some ideas on reducing labels and making up extra variables.\n",
    "\n",
    "Key tasks to implement\n",
    "* Diminish value dimension by combining labels or columns\n",
    "* Add an underscore in front of the column name to indicate a dropped column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.325957Z",
     "iopub.status.busy": "2022-09-12T12:47:06.324875Z",
     "iopub.status.idle": "2022-09-12T12:47:06.435752Z",
     "shell.execute_reply": "2022-09-12T12:47:06.434347Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.325922Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = \"customerID\"\n",
    "df.columns = [\"_\" + col if col in dc else col for col in df.columns]\n",
    "\n",
    "dc = [\"Partner\", \"Dependents\",]\n",
    "check = df[dc].apply(lambda y: \"Yes\" in list(y), axis=1)\n",
    "df[\"Relationship\"] = np.where(check, \"Family\", \"Single\")\n",
    "df.columns = [\"_\" + col if col in dc else col for col in df.columns]\n",
    "\n",
    "dc = \"Contract\"\n",
    "check = df[dc].isin([\"Month-to-month\"])\n",
    "df[dc] = np.where(check, \"No\", \"Yes\")\n",
    "\n",
    "dc = \"PaymentMethod\"\n",
    "check = df[dc].str.contains(\"automatic\")\n",
    "df[dc] = np.where(check, \"Automatic\", \"Manual\")\n",
    "\n",
    "dc = [\n",
    "    \"MultipleLines\", \"OnlineSecurity\", \"OnlineBackup\",\n",
    "    \"DeviceProtection\", \"TechSupport\", \"StreamingTV\",\n",
    "    \"StreamingMovies\",\n",
    "]\n",
    "check = df[dc].isin([\"Yes\", \"No\"])\n",
    "df[dc] = df[dc].where(check, \"No\")\n",
    "\n",
    "check = df[\"MonthlyCharges\"] * df[\"tenure\"] != df[\"TotalCharges\"]\n",
    "df[\"ContractUpdate\"] = np.where(check, \"Yes\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.437707Z",
     "iopub.status.busy": "2022-09-12T12:47:06.437324Z",
     "iopub.status.idle": "2022-09-12T12:47:06.469934Z",
     "shell.execute_reply": "2022-09-12T12:47:06.468686Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.437639Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build machine learning models, we need to provide a well-structured data matrix to the models. The algorithms of each model are out of scope in this notebook, so we just implement the pre-processing step for feeding the learning algorithms. For the high-level idea to train a model, we train a model and evaluate the model using different portions of the same dataset.\n",
    "\n",
    "Key tasks to implement:\n",
    "1. Convert data into proper data type\n",
    "2. Split data into train set and test set for building and evaluating models, respectively\n",
    "3. Scale explanatory variables by their minimum and maximum value\n",
    "4. Duplicate train set with an oversampling method to handle imbalance response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.472425Z",
     "iopub.status.busy": "2022-09-12T12:47:06.471493Z",
     "iopub.status.idle": "2022-09-12T12:47:06.540088Z",
     "shell.execute_reply": "2022-09-12T12:47:06.539039Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.472377Z"
    }
   },
   "outputs": [],
   "source": [
    "dc = \"InternetService\"\n",
    "check = df[dc] == \"DSL\"\n",
    "df[\"DslService\"] = np.where(check, 1, 0)\n",
    "check = df[dc] == \"Fiber optic\"\n",
    "df[\"FiberOpticService\"] = np.where(check, 1, 0)\n",
    "df.columns = [\"_\" + col if col in dc else col for col in df.columns]\n",
    "\n",
    "df = df.apply(\n",
    "    lambda y: LabelEncoder().fit_transform(y) \\\n",
    "    if y.dtype == \"object\" else y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.542514Z",
     "iopub.status.busy": "2022-09-12T12:47:06.541390Z",
     "iopub.status.idle": "2022-09-12T12:47:06.567480Z",
     "shell.execute_reply": "2022-09-12T12:47:06.566227Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.542470Z"
    }
   },
   "outputs": [],
   "source": [
    "dc_y = [\"Churn\",]\n",
    "dc_X = [col for col in df.columns if not (col in dc_y or col.startswith(\"_\"))]\n",
    "dc_oth = [col for col in df.columns if col not in dc_y + dc_X]\n",
    "\n",
    "train, test = train_test_split(\n",
    "    df, test_size=0.3, random_state=852, stratify=df[\"Churn\"]\n",
    ")\n",
    "\n",
    "y_train, X_train, oth_train = \\\n",
    "    train[dc_y].values.reshape(-1), train[dc_X].values, train[dc_oth]\n",
    "y_test, X_test, oth_test = \\\n",
    "    test[dc_y].values.reshape(-1), test[dc_X].values, test[dc_oth]\n",
    "\n",
    "y, X, oth = df[dc_y].values.reshape(-1), df[dc_X].values, df[dc_oth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.569967Z",
     "iopub.status.busy": "2022-09-12T12:47:06.569485Z",
     "iopub.status.idle": "2022-09-12T12:47:06.578612Z",
     "shell.execute_reply": "2022-09-12T12:47:06.577793Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.569923Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.582366Z",
     "iopub.status.busy": "2022-09-12T12:47:06.582024Z",
     "iopub.status.idle": "2022-09-12T12:47:06.670492Z",
     "shell.execute_reply": "2022-09-12T12:47:06.668896Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.582338Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = SMOTE(random_state=852)\n",
    "X_sm, y_sm = sampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.677045Z",
     "iopub.status.busy": "2022-09-12T12:47:06.675957Z",
     "iopub.status.idle": "2022-09-12T12:47:06.744686Z",
     "shell.execute_reply": "2022-09-12T12:47:06.743486Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.676970Z"
    }
   },
   "outputs": [],
   "source": [
    "nrow, ncol = X_train.shape\n",
    "rv_ratio = sum(y_train == 1) / len(y_train) * 100\n",
    "print(f\"Train set has {ncol:,} columns with {nrow:,} rows.\")\n",
    "print(f\"Positive rate of response variable is {rv_ratio:.2f}%.\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "nrow, ncol = X_sm.shape\n",
    "rv_ratio = sum(y_sm == 1) / len(y_sm) * 100\n",
    "print(f\"Train set (oversampling) has {ncol:,} columns with {nrow:,} rows.\")\n",
    "print(f\"Positive rate of response variable is {rv_ratio:.2f}%.\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "nrow, ncol = X_test.shape\n",
    "rv_ratio = sum(y_test == 1) / len(y_test) * 100\n",
    "print(f\"Train set has {ncol:,} columns with {nrow:,} rows.\")\n",
    "print(f\"Positive rate of response variable is {rv_ratio:.2f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.747594Z",
     "iopub.status.busy": "2022-09-12T12:47:06.746027Z",
     "iopub.status.idle": "2022-09-12T12:47:06.753792Z",
     "shell.execute_reply": "2022-09-12T12:47:06.752986Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.747559Z"
    }
   },
   "outputs": [],
   "source": [
    "def estimate_rv(pi_hat, threshold):\n",
    "    \n",
    "    return (pi_hat >= threshold).astype(int)\n",
    "\n",
    "def optimise_threshold(model, X, y):\n",
    "    pi_hat = model.predict_proba(X)[:, 1]\n",
    "    thresholds = np.arange(0, 1, 0.001)\n",
    "    scores = [f1_score(y, estimate_rv(pi_hat, th)) for th in thresholds]\n",
    "    ix = np.argmax(scores)\n",
    "    \n",
    "    return thresholds[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.755874Z",
     "iopub.status.busy": "2022-09-12T12:47:06.755234Z",
     "iopub.status.idle": "2022-09-12T12:47:06.770148Z",
     "shell.execute_reply": "2022-09-12T12:47:06.768880Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.755841Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_model(model, X, y, threshold):\n",
    "    pi_hat = model.predict_proba(X)[:, 1]\n",
    "    y_hat = estimate_rv(pi_hat, threshold)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_hat).ravel()\n",
    "    pos_hat = [tp, fp]\n",
    "    \n",
    "    metrics = np.array([])\n",
    "    metrics = np.append(metrics, accuracy_score(y, y_hat))\n",
    "    metrics = np.append(metrics, precision_score(y, y_hat, average=None))\n",
    "    metrics = np.append(metrics, recall_score(y, y_hat, average=None))\n",
    "    metrics = np.append(metrics, f1_score(y, y_hat, average=None))\n",
    "    metrics = np.append(metrics, pos_hat)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.771947Z",
     "iopub.status.busy": "2022-09-12T12:47:06.771553Z",
     "iopub.status.idle": "2022-09-12T12:47:06.784143Z",
     "shell.execute_reply": "2022-09-12T12:47:06.783026Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.771897Z"
    }
   },
   "outputs": [],
   "source": [
    "def learn_params(model, X, y):\n",
    "    exe_start = time()\n",
    "    model.fit(X, y)\n",
    "    threshold = optimise_threshold(model, X, y)\n",
    "    elapsed_time = time() - exe_start\n",
    "    params = model.best_params_\n",
    "    params.update({\"threshold\": threshold})\n",
    "    params.update({\"time\": elapsed_time})\n",
    "    \n",
    "    return model, params\n",
    "\n",
    "def evaluate_model(tag, model, X_train, y_train, X_test, y_test):\n",
    "    model, params = learn_params(model, X_train, y_train)\n",
    "    evaluation = score_model(model, X_test, y_test, params[\"threshold\"])\n",
    "    \n",
    "    idx_time = [\"Time (sec)\",]\n",
    "    idx_params = [\"Best Params\",]\n",
    "    idx_metrics = [\n",
    "        \"Accuracy\",\n",
    "        \"Precision (0)\", \"Precision (1)\",\n",
    "        \"Recall (0)\", \"Recall (1)\",\n",
    "        \"F1-score (0)\", \"F1-score (1)\",\n",
    "    ]\n",
    "    idx_cm = [\"True Positive\", \"False Positive\",]\n",
    "    \n",
    "    time_ = pd.Series(params.pop(\"time\"), index=idx_time)\n",
    "    params_ = pd.Series(str(params), index=idx_params)\n",
    "    eval_ = pd.Series(evaluation, index=idx_metrics + idx_cm)\n",
    "    \n",
    "    output = pd.concat([time_, eval_, params_]).to_frame().T\n",
    "    output.index = [tag]\n",
    "    output[idx_time + idx_metrics] = output[idx_time + idx_metrics] \\\n",
    "        .astype(float) \\\n",
    "        .round(2)\n",
    "    output[idx_cm] = output[idx_cm].astype(int)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.786057Z",
     "iopub.status.busy": "2022-09-12T12:47:06.785703Z",
     "iopub.status.idle": "2022-09-12T12:47:06.801987Z",
     "shell.execute_reply": "2022-09-12T12:47:06.800774Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.786014Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseModel():\n",
    "    def __init__(self):\n",
    "        self.best_params_ = {}\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.EY = np.mean(y)\n",
    "        self.VY = np.var(y)\n",
    "        \n",
    "        self.alpha = ((1 - self.EY) / self.VY - 1 / self.EY) * self.EY ** 2\n",
    "        self.beta = self.alpha * (1 / self.EY - 1)\n",
    "        \n",
    "        self.best_params_[\"alpha\"] = self.alpha\n",
    "        self.best_params_[\"beta\"] = self.beta\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        np.random.seed(852)\n",
    "        pi_hat = np.random.beta(self.alpha, self.beta, X.shape[0])\n",
    "        pi_hat = pd.DataFrame({\"0\": pi_hat, \"1\": pi_hat}).values\n",
    "        \n",
    "        return pi_hat\n",
    "\n",
    "model_bm = BaseModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.804404Z",
     "iopub.status.busy": "2022-09-12T12:47:06.803733Z",
     "iopub.status.idle": "2022-09-12T12:47:06.813675Z",
     "shell.execute_reply": "2022-09-12T12:47:06.812839Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.804370Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    \"C\": [0.001, 0.01, 0.1 , 1, 10, 100, 1000],\n",
    "    \"class_weight\": [\"auto\"],\n",
    "    \"max_iter\": [10000],\n",
    "    \"penalty\": [\"elasticnet\"],\n",
    "    \"solver\": [\"saga\"],\n",
    "    \"l1_ratio\": [0, 0.2, 0.4, 0.6, 0.8, 1],\n",
    "    \"random_state\": [852],\n",
    "}\n",
    "\n",
    "rs_lr_params = {\n",
    "    \"estimator\": LogisticRegression(),\n",
    "    \"param_distributions\": lr_params,\n",
    "    \"scoring\": \"roc_auc\",\n",
    "    \"cv\": 10,\n",
    "    \"verbose\": 1,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "model_lr = RandomizedSearchCV(**rs_lr_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.816189Z",
     "iopub.status.busy": "2022-09-12T12:47:06.815440Z",
     "iopub.status.idle": "2022-09-12T12:47:06.829949Z",
     "shell.execute_reply": "2022-09-12T12:47:06.828918Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.816146Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\" : [50, 100, 150, 200],\n",
    "    \"criterion\" : [\"gini\", \"entropy\"],\n",
    "    \"max_depth\" : [None, 1, 2, 5, 7, 12],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"random_state\": [852],\n",
    "}\n",
    "\n",
    "rs_rf_params = {\n",
    "    \"estimator\": RandomForestClassifier(),\n",
    "    \"param_distributions\": rf_params,\n",
    "    \"scoring\": \"roc_auc\",\n",
    "    \"cv\": 10,\n",
    "    \"verbose\": 1,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "model_rf = RandomizedSearchCV(**rs_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:47:06.831640Z",
     "iopub.status.busy": "2022-09-12T12:47:06.831048Z",
     "iopub.status.idle": "2022-09-12T12:49:47.060481Z",
     "shell.execute_reply": "2022-09-12T12:49:47.058852Z",
     "shell.execute_reply.started": "2022-09-12T12:47:06.831607Z"
    }
   },
   "outputs": [],
   "source": [
    "summary = pd.concat([\n",
    "    evaluate_model(\"Random Guess\", model_bm, X_train, y_train, X_test, y_test),\n",
    "    evaluate_model(\"Logistic Regression\", model_lr, X_train, y_train, X_test, y_test),\n",
    "    evaluate_model(\"Logistic Regression (Oversampling)\", model_lr, X_sm, y_sm, X_test, y_test),\n",
    "    evaluate_model(\"Random Forest\", model_rf, X_train, y_train, X_test, y_test),\n",
    "    evaluate_model(\"Random Forest (Oversampling)\", model_rf, X_sm, y_sm, X_test, y_test),\n",
    "])\n",
    "\n",
    "summary_coloured = summary.style.background_gradient(cmap='YlOrRd', axis=0, low=0.1, high=1.0)\n",
    "display(summary_coloured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above summary of model evaluation, the `Random Guess` model is underperforming while other models have similar performance. For imbalance in the response variable, oversampling method slightly improved `Logistic Regression (Oversampling)` in terms of F1-score. However, business users only being persuaded when they see some nice figure in performance. Among the fitted models, `Random Forest` outperformed in precision and will be selected as the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <span style=\"font-size:18px;\">\n",
    "        Thanks for your time reading through this notebook! This is our first data science project ever and may contain conceptual errors. That's why we would appreciate your comment and feedback for improvement. Upvoting this notebook would also motivate us to learn more and work more if you enjoyed this notebook.\n",
    "    <\\span>\n",
    "<\\div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Understand the Service Charges\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:49:47.062521Z",
     "iopub.status.busy": "2022-09-12T12:49:47.062166Z",
     "iopub.status.idle": "2022-09-12T12:49:47.123429Z",
     "shell.execute_reply": "2022-09-12T12:49:47.122229Z",
     "shell.execute_reply.started": "2022-09-12T12:49:47.062482Z"
    }
   },
   "outputs": [],
   "source": [
    "service = [\n",
    "    \"PhoneService\",\n",
    "    \"MultipleLines\",\n",
    "    \"DslService\", \"FiberOpticService\",\n",
    "    \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n",
    "    \"StreamingTV\", \"StreamingMovies\",\n",
    "]\n",
    "\n",
    "Y = df[\"MonthlyCharges\"]\n",
    "X = df[service]\n",
    "model = sm.OLS(Y,X)\n",
    "results = model.fit()\n",
    "\n",
    "results.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Putting Model to Production\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-12T12:49:47.126248Z",
     "iopub.status.busy": "2022-09-12T12:49:47.125394Z",
     "iopub.status.idle": "2022-09-12T12:49:47.978961Z",
     "shell.execute_reply": "2022-09-12T12:49:47.977740Z",
     "shell.execute_reply.started": "2022-09-12T12:49:47.126202Z"
    }
   },
   "outputs": [],
   "source": [
    "model_params = summary.loc[\"Random Forest\", \"Best Params\"]\n",
    "model_params = ast.literal_eval(model_params)\n",
    "threshold = model_params.pop(\"threshold\")\n",
    "\n",
    "model = RandomForestClassifier(**model_params)\n",
    "model.fit(X_train, y_train)\n",
    "pi_hat = model.predict_proba(X_test)[:, 1]\n",
    "y_hat = estimate_rv(pi_hat, threshold)\n",
    "\n",
    "df_deck = test.assign(ChurnHat=y_hat).copy()\n",
    "df_deck.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
